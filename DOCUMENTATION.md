# RAGVersion: Complete Documentation & User Guide

**Version 0.1.0** | **Async-first version tracking for RAG applications**

---

## Table of Contents

1. [Introduction](#introduction)
2. [The Problem RAGVersion Solves](#the-problem-ragversion-solves)
3. [Why We Built RAGVersion](#why-we-built-ragversion)
4. [How RAGVersion Helps Developers](#how-ragversion-helps-developers)
5. [Installation & Setup](#installation--setup)
6. [Core Concepts](#core-concepts)
7. [Getting Started](#getting-started)
8. [Complete Feature Guide](#complete-feature-guide)
9. [Integration Guides](#integration-guides)
10. [CLI Reference](#cli-reference)
11. [API Reference](#api-reference)
12. [Advanced Use Cases](#advanced-use-cases)
13. [Best Practices](#best-practices)
14. [Troubleshooting](#troubleshooting)
15. [Architecture Deep Dive](#architecture-deep-dive)

---

## Introduction

### What is RAGVersion?

RAGVersion is a production-ready, async-first version tracking system specifically designed for Retrieval-Augmented Generation (RAG) applications. It solves the critical problem of keeping your vector databases in sync with changing source documents.

### Key Features at a Glance

- **Async-first architecture** - Built from the ground up for Python's async/await patterns
- **Automatic change detection** - Uses content hashing to detect when documents change
- **Version history** - Complete audit trail of all document changes
- **Content diffing** - See exactly what changed between versions
- **Batch processing** - Efficiently process thousands of documents
- **Framework integrations** - Ready-to-use helpers for LangChain and LlamaIndex
- **Production-ready** - Resilient error handling and continue-on-error design
- **Storage backend** - Powered by Supabase/PostgreSQL for reliability
- **Document parsing** - Built-in support for PDF, DOCX, TXT, Markdown, and more

---

## The Problem RAGVersion Solves

### The RAG Synchronization Challenge

When building RAG applications, developers face a critical challenge: **keeping vector databases synchronized with changing source documents**.

#### Common Problems:

**1. Stale Embeddings**
```
Your documents change ‚Üí Your vector DB doesn't update ‚Üí Users get outdated information
```

**2. Over-Indexing**
```
Re-indexing all documents ‚Üí Expensive API calls ‚Üí Wasted compute ‚Üí Slow updates
```

**3. No Change Detection**
```
Which documents changed? ‚Üí Manual tracking ‚Üí Error-prone ‚Üí Time-consuming
```

**4. No Version History**
```
Document was updated ‚Üí What changed? ‚Üí No audit trail ‚Üí Can't debug issues
```

**5. Scaling Problems**
```
Processing 10,000 documents ‚Üí One failure breaks entire batch ‚Üí No resilience
```

### Real-World Scenario

Imagine you're building a customer support RAG system:

```
Day 1: You index 1,000 product documentation files
Day 5: Product team updates 50 files
Day 10: 25 more files are updated
Day 15: Someone asks "What's the latest pricing?"

‚ùå Without RAGVersion:
   - You don't know which 75 files changed
   - You re-index all 1,000 files (expensive!)
   - Or risk serving outdated information
   - No way to audit what changed when

‚úÖ With RAGVersion:
   - Automatically detects the 75 changed files
   - Only re-index those 75 files
   - Complete version history for auditing
   - View diffs to see exact changes
```

---

## Why We Built RAGVersion

### The Genesis

RAGVersion was born from real production challenges faced when building large-scale RAG applications. We identified several pain points:

#### 1. **Lack of Specialized Tools**

Existing version control systems (Git, DVC, etc.) weren't designed for RAG workflows:
- Git tracks code, not document content changes for RAG
- DVC is for ML models, not document versioning
- No tools specifically for the RAG use case

#### 2. **Complex Integration**

Every RAG application required custom change detection:
```python
# Before RAGVersion - Custom implementation
def track_changes():
    # Read file
    # Compute hash
    # Store in database
    # Compare with previous
    # Handle errors
    # Update vector DB
    # ... 200+ lines of custom code
```

#### 3. **Production Requirements**

RAG systems in production need:
- **Reliability**: Continue processing even if some documents fail
- **Observability**: Know what changed, when, and why
- **Efficiency**: Only process changed documents
- **Scalability**: Handle thousands of documents
- **Audit trails**: Compliance and debugging

#### 4. **Framework Fragmentation**

Different RAG frameworks (LangChain, LlamaIndex, custom) all need change tracking, but no unified solution existed.

### Design Philosophy

RAGVersion was built with these principles:

1. **Async-First**: Modern Python is async, so we are too
2. **Plug-and-Play**: Works with any RAG framework
3. **Production-Ready**: Resilient, observable, scalable
4. **Developer-Friendly**: Simple API, comprehensive docs
5. **Flexible**: Use what you need, extend what you want

---

## How RAGVersion Helps Developers

### For RAG Application Developers

#### 1. **Save Money on API Calls**

```python
# Without RAGVersion: Re-index everything
# Cost: 1000 documents √ó $0.0001/token √ó 500 tokens = $50

# With RAGVersion: Only index changed documents
# Cost: 10 changed documents √ó $0.0001/token √ó 500 tokens = $0.50

# Savings: 99% cost reduction
```

#### 2. **Faster Update Cycles**

```python
# Without RAGVersion
time_to_update = total_documents √ó processing_time
# 1000 docs √ó 2 seconds = 33 minutes

# With RAGVersion
time_to_update = changed_documents √ó processing_time
# 10 docs √ó 2 seconds = 20 seconds

# 99x faster updates
```

#### 3. **Reliable Production Systems**

```python
# Resilient batch processing
result = await tracker.track_directory("./docs")

if result.success_rate < 0.95:
    alert_ops_team(result.failed)

# Continue processing even if some files fail
```

#### 4. **Debugging & Auditing**

```python
# View complete history
versions = await tracker.list_versions(doc_id)

# See exact changes
diff = await tracker.get_diff(doc_id, v1, v2)
print(f"Changes: +{diff.additions} -{diff.deletions}")

# Audit trail for compliance
```

### For Enterprise Teams

#### 1. **Compliance & Governance**

- Complete audit trail of all document changes
- Track who changed what and when
- Version history for regulatory requirements

#### 2. **Cost Optimization**

- Reduce embedding API costs by 90%+
- Optimize compute resources
- Lower infrastructure costs

#### 3. **Operational Excellence**

- Automated change detection
- Error reporting and monitoring
- Scalable batch processing

### For Individual Developers

#### 1. **Focus on Your RAG Logic**

```python
# You focus on this:
def build_rag_pipeline():
    loader = DocumentLoader()
    splitter = TextSplitter()
    embedder = OpenAIEmbeddings()
    # ... your RAG logic

# RAGVersion handles this:
# - Change detection
# - Version tracking
# - Storage management
# - Error handling
# - Batch processing
```

#### 2. **Rapid Prototyping**

```python
# Get started in 3 lines
tracker = AsyncVersionTracker(storage=SupabaseStorage.from_env())
changes = await tracker.track_directory("./docs")
print(f"Found {len(changes.successful)} changes")
```

#### 3. **Production-Grade Features**

- No need to build your own change detection
- Battle-tested in production
- Comprehensive error handling included

---

## Installation & Setup

### Prerequisites

- Python 3.9 or higher
- Supabase account (free tier works)
- Basic understanding of async/await in Python

### Installation Options

#### Basic Installation

```bash
pip install ragversion
```

Includes:
- Core tracking functionality
- Supabase storage backend
- Basic file type support

#### With Document Parsers

```bash
pip install "ragversion[parsers]"
```

Adds support for:
- PDF files (pypdf)
- DOCX files (python-docx)
- PPTX files (python-pptx)
- Excel files (openpyxl)
- Enhanced Markdown parsing

#### With LangChain Integration

```bash
pip install "ragversion[langchain]"
```

Includes:
- LangChain compatibility helpers
- Document loader integration
- Vector store sync utilities

#### With LlamaIndex Integration

```bash
pip install "ragversion[llamaindex]"
```

Includes:
- LlamaIndex compatibility helpers
- Node parser integration
- Index sync utilities

#### Complete Installation (Recommended for Development)

```bash
pip install "ragversion[all]"
```

Includes everything:
- All parsers
- All integrations
- Development tools (pytest, black, ruff, mypy)

### Supabase Setup

#### Step 1: Create a Supabase Project

1. Go to [supabase.com](https://supabase.com)
2. Create a free account
3. Create a new project
4. Note your project URL and service key

#### Step 2: Run Database Migration

1. Initialize RAGVersion in your project:
```bash
ragversion init
```

2. Edit `ragversion.yaml` with your Supabase credentials:
```yaml
storage:
  backend: supabase
  supabase:
    url: https://your-project.supabase.co
    key: your-service-key
```

3. Get the migration SQL:
```bash
ragversion migrate
```

4. Copy the SQL and run it in your Supabase SQL Editor:
   - Go to SQL Editor in Supabase dashboard
   - Paste the migration SQL
   - Run the query

#### Step 3: Verify Setup

```bash
ragversion health
```

You should see: ‚úì Storage backend is healthy

### Environment Variables

For production, use environment variables instead of YAML:

```bash
export SUPABASE_URL="https://your-project.supabase.co"
export SUPABASE_SERVICE_KEY="your-service-key"
```

Add to your `.env` file:
```env
SUPABASE_URL=https://your-project.supabase.co
SUPABASE_SERVICE_KEY=your-service-key
```

---

## Core Concepts

### 1. Documents

A **document** is any file you want to track:
- PDF files
- Word documents (DOCX)
- Text files
- Markdown files
- PowerPoint presentations
- Excel spreadsheets

Each document has:
- **Unique ID**: UUID identifier
- **File path**: Location of the document
- **File hash**: Content hash for change detection
- **Metadata**: File size, type, timestamps
- **Version count**: Number of versions

### 2. Versions

Every time a document changes, a new **version** is created:
- **Version number**: Sequential (1, 2, 3...)
- **Content hash**: Hash of the content
- **Change type**: created | modified | deleted
- **Timestamp**: When the change occurred
- **Content**: Optional stored content (compressed)

### 3. Change Detection

RAGVersion uses **content hashing** to detect changes:

```python
# How it works
old_hash = sha256(previous_content)
new_hash = sha256(current_content)

if old_hash != new_hash:
    # Document changed, create new version
    create_version(document, new_content)
```

This approach:
- ‚úì Detects any content change
- ‚úì Ignores metadata-only changes
- ‚úì Fast comparison (no content diff needed)
- ‚úì Works with any file type

### 4. Batch Processing

**Batch processing** allows you to track multiple documents efficiently:

```python
result = await tracker.track_directory(
    "./documents",
    patterns=["*.pdf", "*.docx"],
    recursive=True,
    max_workers=4  # Parallel processing
)
```

Features:
- **Parallel processing**: Multiple workers
- **Continue-on-error**: One failure doesn't stop the batch
- **Progress tracking**: Monitor progress
- **Error reporting**: Detailed error information

### 5. Change Events

When a document changes, **change events** are emitted:

```python
async def on_change(event: ChangeEvent):
    print(f"Document {event.file_name} was {event.change_type}")
    # Trigger re-indexing, send notification, etc.

tracker.on_change(on_change)
```

Event types:
- `created`: New document
- `modified`: Existing document changed
- `deleted`: Document removed

### 6. Storage Backend

RAGVersion stores data in a **storage backend**:

- **Primary**: Supabase (PostgreSQL)
- **Future**: Local SQLite, MongoDB, etc.
- **Testing**: MockStorage (in-memory)

Storage handles:
- Document metadata
- Version history
- Content storage (optional)
- Querying and filtering

---

## Getting Started

### Your First RAG Version Tracking

Let's build a simple example that tracks documents and detects changes.

#### Step 1: Initialize Tracker

```python
import asyncio
from ragversion import AsyncVersionTracker
from ragversion.storage import SupabaseStorage

async def main():
    # Create tracker
    tracker = AsyncVersionTracker(
        storage=SupabaseStorage.from_env()
    )

    # Initialize storage connection
    await tracker.initialize()

    # Your code here...

    # Clean up
    await tracker.close()

asyncio.run(main())
```

#### Step 2: Track a Single Document

```python
# Track a single file
change = await tracker.track("./docs/readme.pdf")

if change:
    print(f"Change detected: {change.change_type}")
    print(f"Version: {change.version_number}")
    print(f"Hash: {change.content_hash}")
else:
    print("No changes detected")
```

#### Step 3: Track Multiple Documents

```python
# Track all PDFs in a directory
result = await tracker.track_directory(
    "./docs",
    patterns=["*.pdf"],
    recursive=True
)

print(f"Total files: {result.total_files}")
print(f"Changes detected: {result.success_count}")
print(f"Failures: {result.failure_count}")
print(f"Duration: {result.duration_seconds:.2f}s")
```

#### Step 4: View Document History

```python
# List all tracked documents
documents = await tracker.list_documents(limit=10)

for doc in documents:
    print(f"{doc.file_name}: {doc.version_count} versions")

# Get versions for a specific document
document_id = documents[0].id
versions = await tracker.list_versions(document_id)

for version in versions:
    print(f"v{version.version_number}: {version.change_type} at {version.created_at}")
```

#### Step 5: Compare Versions

```python
# Get diff between two versions
diff = await tracker.get_diff(
    document_id,
    from_version=1,
    to_version=2
)

print(f"Additions: {diff.additions}")
print(f"Deletions: {diff.deletions}")
print(f"Changes:\n{diff.diff_text}")
```

### Complete Example: Document Sync System

Here's a complete example of a document sync system:

```python
import asyncio
from pathlib import Path
from ragversion import AsyncVersionTracker
from ragversion.storage import SupabaseStorage

async def sync_documents(directory: str):
    """Sync all documents in a directory."""

    # Initialize tracker
    tracker = AsyncVersionTracker(
        storage=SupabaseStorage.from_env(),
        store_content=True,  # Store document content
        max_file_size_mb=50  # Skip files > 50MB
    )

    await tracker.initialize()

    try:
        # Track all documents
        result = await tracker.track_directory(
            directory,
            patterns=["*.pdf", "*.docx", "*.txt", "*.md"],
            recursive=True,
            max_workers=4
        )

        # Report results
        print(f"\nüìä Sync Results:")
        print(f"   Total files: {result.total_files}")
        print(f"   Changes detected: {result.success_count}")
        print(f"   Errors: {result.failure_count}")
        print(f"   Duration: {result.duration_seconds:.2f}s")
        print(f"   Success rate: {result.success_rate:.1%}")

        # Show changes
        if result.successful:
            print(f"\n‚ú® Changes:")
            for event in result.successful:
                icon = "üÜï" if event.change_type.value == "created" else "üìù"
                print(f"   {icon} {event.file_name} (v{event.version_number})")

        # Show errors
        if result.failed:
            print(f"\n‚ùå Errors:")
            for error in result.failed:
                print(f"   {error.file_path}: {error.error}")

        return result

    finally:
        await tracker.close()

# Run the sync
if __name__ == "__main__":
    asyncio.run(sync_documents("./my_documents"))
```

---

## Complete Feature Guide

### Feature 1: Change Detection

#### How It Works

```python
# First time tracking a document
change1 = await tracker.track("document.pdf")
# Result: ChangeEvent(change_type="created", version=1)

# Document unchanged
change2 = await tracker.track("document.pdf")
# Result: None (no change)

# Modify the document, then track again
change3 = await tracker.track("document.pdf")
# Result: ChangeEvent(change_type="modified", version=2)
```

#### Advanced Options

```python
# Track with custom metadata
change = await tracker.track(
    "document.pdf",
    metadata={"author": "John Doe", "department": "Engineering"}
)

# Skip content storage (only track metadata)
tracker = AsyncVersionTracker(
    storage=storage,
    store_content=False  # Save storage space
)
```

### Feature 2: Batch Processing

#### Basic Batch Processing

```python
# Process all files in a directory
result = await tracker.track_directory("./docs")
```

#### Advanced Batch Processing

```python
result = await tracker.track_directory(
    "./docs",
    patterns=["*.pdf", "*.docx"],  # Multiple patterns
    recursive=True,                # Include subdirectories
    max_workers=8,                 # Parallel workers
    on_error="continue"            # Continue on errors
)
```

#### Custom Progress Callback

```python
async def progress_callback(current: int, total: int):
    progress = (current / total) * 100
    print(f"Progress: {progress:.1f}% ({current}/{total})")

result = await tracker.track_directory(
    "./docs",
    progress_callback=progress_callback
)
```

### Feature 3: Version History

#### List All Documents

```python
# Get all documents
documents = await tracker.list_documents()

# Get documents with filtering
documents = await tracker.list_documents(
    limit=20,
    offset=0,
    order_by="updated_at",
    order="desc"
)

# Filter by file type
pdf_docs = [doc for doc in documents if doc.file_type == "pdf"]
```

#### List Document Versions

```python
# Get all versions of a document
versions = await tracker.list_versions(document_id)

# Get specific version
version = await tracker.get_version(document_id, version_number=2)

# Get latest version
latest = await tracker.get_latest_version(document_id)
```

### Feature 4: Content Diffing

#### Basic Diff

```python
# Compare two versions
diff = await tracker.get_diff(
    document_id,
    from_version=1,
    to_version=2
)

print(f"Added lines: {diff.additions}")
print(f"Removed lines: {diff.deletions}")
print(f"Diff:\n{diff.diff_text}")
```

#### Diff Output Format

```
@@ -1,5 +1,5 @@
 This is line 1
-This is the old line 2
+This is the new line 2
 This is line 3
+This is a new line 4
```

### Feature 5: Event System

#### Register Event Callbacks

```python
# Single callback
async def on_change(event: ChangeEvent):
    print(f"Change: {event.file_name}")

tracker.on_change(on_change)

# Multiple callbacks
async def log_change(event: ChangeEvent):
    logger.info(f"Document changed: {event.file_name}")

async def trigger_reindex(event: ChangeEvent):
    await vector_db.reindex(event.document_id)

tracker.on_change(log_change)
tracker.on_change(trigger_reindex)
```

#### Event Types

```python
from ragversion.models import ChangeType

# Filter by change type
async def on_new_document(event: ChangeEvent):
    if event.change_type == ChangeType.CREATED:
        await process_new_document(event)

tracker.on_change(on_new_document)
```

### Feature 6: Content Storage

#### Enable Content Storage

```python
# Store document content in database
tracker = AsyncVersionTracker(
    storage=storage,
    store_content=True
)
```

#### Retrieve Stored Content

```python
# Get content of a specific version
content = await tracker.get_content(
    document_id,
    version_number=2
)

# Content is automatically decompressed
print(content)  # Original document content
```

#### Storage Options

```python
# Configure compression
tracker = AsyncVersionTracker(
    storage=storage,
    store_content=True,
    compression="gzip"  # or "none"
)
```

### Feature 7: File Size Limits

```python
# Skip files larger than 50MB
tracker = AsyncVersionTracker(
    storage=storage,
    max_file_size_mb=50
)

# Files over limit are skipped with warning
result = await tracker.track_directory("./docs")
# Large files appear in result.failed with error_type="file_too_large"
```

### Feature 8: Context Manager Support

```python
# Automatic initialization and cleanup
async with AsyncVersionTracker(storage=storage) as tracker:
    result = await tracker.track_directory("./docs")
    # Automatically calls initialize() and close()
```

---

## Integration Guides

### LangChain Integration

#### Overview

The LangChain integration provides automatic synchronization between your documents and LangChain vector stores.

#### Setup

```python
from ragversion.integrations.langchain import LangChainSync
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import Qdrant
from qdrant_client import QdrantClient

# Initialize components
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,
    chunk_overlap=200
)

embeddings = OpenAIEmbeddings()

qdrant_client = QdrantClient(url="http://localhost:6333")
vectorstore = Qdrant(
    client=qdrant_client,
    collection_name="my_documents",
    embeddings=embeddings
)

# Initialize tracker
tracker = AsyncVersionTracker(storage=SupabaseStorage.from_env())

# Create sync helper
sync = LangChainSync(
    tracker=tracker,
    text_splitter=text_splitter,
    embeddings=embeddings,
    vectorstore=vectorstore
)
```

#### Sync Directory

```python
# Sync all documents
await sync.sync_directory(
    "./documents",
    patterns=["*.pdf", "*.docx"],
    recursive=True
)
```

#### How It Works

1. **Detects changes**: Uses RAGVersion to find changed documents
2. **Removes old embeddings**: Deletes outdated embeddings from vector store
3. **Processes documents**: Loads and splits new/modified documents
4. **Creates embeddings**: Generates embeddings for chunks
5. **Updates vector store**: Adds new embeddings to vector store

#### Advanced Usage

```python
# Custom document loader
from langchain.document_loaders import PyPDFLoader

sync = LangChainSync(
    tracker=tracker,
    text_splitter=text_splitter,
    embeddings=embeddings,
    vectorstore=vectorstore,
    document_loader=PyPDFLoader  # Custom loader
)

# Sync with metadata
await sync.sync_directory(
    "./documents",
    metadata={"source": "documentation", "version": "2.0"}
)
```

### LlamaIndex Integration

#### Overview

The LlamaIndex integration provides automatic synchronization with LlamaIndex vector stores.

#### Setup

```python
from ragversion.integrations.llamaindex import LlamaIndexSync
from llama_index.core import VectorStoreIndex, StorageContext
from llama_index.core.node_parser import SentenceSplitter
from llama_index.vector_stores import QdrantVectorStore
from qdrant_client import QdrantClient

# Initialize components
node_parser = SentenceSplitter(
    chunk_size=1024,
    chunk_overlap=20
)

qdrant_client = QdrantClient(url="http://localhost:6333")
vector_store = QdrantVectorStore(
    client=qdrant_client,
    collection_name="my_documents"
)

storage_context = StorageContext.from_defaults(
    vector_store=vector_store
)

index = VectorStoreIndex.from_vector_store(
    vector_store,
    storage_context=storage_context
)

# Initialize tracker
tracker = AsyncVersionTracker(storage=SupabaseStorage.from_env())

# Create sync helper
sync = LlamaIndexSync(
    tracker=tracker,
    node_parser=node_parser,
    index=index
)
```

#### Sync Directory

```python
# Sync all documents
await sync.sync_directory(
    "./documents",
    patterns=["*.pdf", "*.docx"],
    recursive=True
)
```

#### How It Works

1. **Detects changes**: Uses RAGVersion to find changed documents
2. **Removes old nodes**: Deletes outdated nodes from index
3. **Loads documents**: Uses LlamaIndex document loaders
4. **Parses nodes**: Creates nodes with node parser
5. **Updates index**: Inserts new nodes into index

#### Advanced Usage

```python
# Custom document loading
from llama_index.core import SimpleDirectoryReader

sync = LlamaIndexSync(
    tracker=tracker,
    node_parser=node_parser,
    index=index,
    document_reader=SimpleDirectoryReader  # Custom reader
)

# Sync with transformations
from llama_index.core.extractors import TitleExtractor

sync = LlamaIndexSync(
    tracker=tracker,
    node_parser=node_parser,
    index=index,
    transformations=[TitleExtractor()]
)
```

### Custom Integration

#### Building Your Own Integration

```python
from ragversion import AsyncVersionTracker
from ragversion.models import ChangeEvent

class CustomRAGSync:
    def __init__(self, tracker: AsyncVersionTracker, vector_db):
        self.tracker = tracker
        self.vector_db = vector_db

        # Register callback
        self.tracker.on_change(self.handle_change)

    async def handle_change(self, event: ChangeEvent):
        """Handle document changes."""

        if event.change_type == "deleted":
            # Remove from vector DB
            await self.vector_db.delete(event.document_id)

        elif event.change_type in ["created", "modified"]:
            # Remove old embeddings
            if event.change_type == "modified":
                await self.vector_db.delete(event.document_id)

            # Load document
            content = await self.load_document(event.file_path)

            # Process and embed
            chunks = self.split_text(content)
            embeddings = await self.embed(chunks)

            # Store in vector DB
            await self.vector_db.insert(
                document_id=event.document_id,
                embeddings=embeddings,
                metadata={"version": event.version_number}
            )

    async def sync_directory(self, directory: str):
        """Sync all documents in directory."""
        result = await self.tracker.track_directory(directory)
        return result
```

---

## CLI Reference

### Installation Verification

```bash
ragversion --version
```

### Initialize Project

```bash
# Create ragversion.yaml configuration
ragversion init

# With custom config path
ragversion init --config my-config.yaml
```

### Database Migration

```bash
# Display migration SQL
ragversion migrate

# With custom config
ragversion migrate --config my-config.yaml
```

### Track Files

```bash
# Track single file
ragversion track ./document.pdf

# Track directory
ragversion track ./documents

# Track with patterns
ragversion track ./documents --patterns "*.pdf" --patterns "*.docx"

# Non-recursive
ragversion track ./documents --no-recursive

# Custom workers
ragversion track ./documents --workers 8
```

### List Documents

```bash
# List all documents
ragversion list

# Limit results
ragversion list --limit 50

# With custom config
ragversion list --config my-config.yaml
```

### View History

```bash
# View document versions
ragversion history <document-id>

# Example
ragversion history 550e8400-e29b-41d4-a716-446655440000
```

### Compare Versions

```bash
# View diff between versions
ragversion diff <document-id> --from-version 1 --to-version 2

# Short form
ragversion diff <document-id> -f 1 -t 2
```

### Health Check

```bash
# Check storage backend health
ragversion health

# With custom config
ragversion health --config my-config.yaml
```

---

## API Reference

### AsyncVersionTracker

Main class for tracking document versions.

#### Constructor

```python
AsyncVersionTracker(
    storage: StorageBackend,
    store_content: bool = True,
    max_file_size_mb: int = 100,
    compression: str = "gzip"
)
```

**Parameters:**
- `storage`: Storage backend instance
- `store_content`: Whether to store document content
- `max_file_size_mb`: Maximum file size to process
- `compression`: Compression algorithm ("gzip" or "none")

#### Methods

##### initialize()

```python
async def initialize() -> None
```

Initialize the tracker and storage backend.

##### close()

```python
async def close() -> None
```

Close the tracker and cleanup resources.

##### track()

```python
async def track(
    file_path: str | Path,
    metadata: dict | None = None
) -> ChangeEvent | None
```

Track a single file for changes.

**Parameters:**
- `file_path`: Path to the file
- `metadata`: Optional metadata dictionary

**Returns:**
- `ChangeEvent` if changes detected, `None` otherwise

##### track_directory()

```python
async def track_directory(
    directory: str | Path,
    patterns: list[str] = ["*"],
    recursive: bool = True,
    max_workers: int = 4
) -> BatchResult
```

Track all files in a directory.

**Parameters:**
- `directory`: Directory path
- `patterns`: File patterns to match
- `recursive`: Include subdirectories
- `max_workers`: Number of parallel workers

**Returns:**
- `BatchResult` with processing results

##### list_documents()

```python
async def list_documents(
    limit: int = 100,
    offset: int = 0
) -> list[Document]
```

List tracked documents.

**Parameters:**
- `limit`: Maximum results
- `offset`: Results offset

**Returns:**
- List of `Document` objects

##### list_versions()

```python
async def list_versions(
    document_id: UUID
) -> list[Version]
```

List versions of a document.

**Parameters:**
- `document_id`: Document UUID

**Returns:**
- List of `Version` objects

##### get_version()

```python
async def get_version(
    document_id: UUID,
    version_number: int
) -> Version | None
```

Get a specific version.

**Parameters:**
- `document_id`: Document UUID
- `version_number`: Version number

**Returns:**
- `Version` object or `None`

##### get_diff()

```python
async def get_diff(
    document_id: UUID,
    from_version: int,
    to_version: int
) -> DiffResult | None
```

Get diff between two versions.

**Parameters:**
- `document_id`: Document UUID
- `from_version`: Starting version
- `to_version`: Ending version

**Returns:**
- `DiffResult` object or `None`

##### on_change()

```python
def on_change(
    callback: Callable[[ChangeEvent], Awaitable[None]]
) -> None
```

Register a change event callback.

**Parameters:**
- `callback`: Async callback function

### Models

#### Document

```python
class Document(BaseModel):
    id: UUID
    file_path: str
    file_name: str
    file_type: str
    file_size: int
    content_hash: str
    version_count: int
    created_at: datetime
    updated_at: datetime
```

#### Version

```python
class Version(BaseModel):
    id: UUID
    document_id: UUID
    version_number: int
    content_hash: str
    change_type: ChangeType
    file_size: int
    content: bytes | None
    created_at: datetime
```

#### ChangeEvent

```python
class ChangeEvent(BaseModel):
    document_id: UUID
    file_path: str
    file_name: str
    version_number: int
    change_type: ChangeType
    content_hash: str
    created_at: datetime
```

#### BatchResult

```python
class BatchResult(BaseModel):
    total_files: int
    success_count: int
    failure_count: int
    duration_seconds: float
    successful: list[ChangeEvent]
    failed: list[FileProcessingError]
    success_rate: float
```

#### DiffResult

```python
class DiffResult(BaseModel):
    document_id: UUID
    from_version: int
    to_version: int
    additions: int
    deletions: int
    diff_text: str
```

---

## Advanced Use Cases

### Use Case 1: Scheduled Document Sync

```python
#!/usr/bin/env python3
"""Cron job to sync documents hourly."""

import asyncio
import logging
from ragversion import AsyncVersionTracker, SupabaseStorage

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

async def hourly_sync():
    tracker = AsyncVersionTracker(storage=SupabaseStorage.from_env())

    await tracker.initialize()

    try:
        result = await tracker.track_directory(
            "/data/documents",
            patterns=["*.pdf", "*.docx", "*.txt"],
            recursive=True,
            max_workers=8
        )

        logger.info(f"Sync completed: {result.success_count} changes")

        if result.failure_count > 0:
            logger.warning(f"Failures: {result.failure_count}")
            for error in result.failed:
                logger.error(f"  {error.file_path}: {error.error}")

        return result.success_count

    finally:
        await tracker.close()

if __name__ == "__main__":
    changes = asyncio.run(hourly_sync())
    print(f"Processed {changes} changes")
```

Add to crontab:
```bash
0 * * * * /path/to/venv/bin/python /path/to/hourly_sync.py
```

### Use Case 2: Real-Time RAG Pipeline

```python
import asyncio
from ragversion import AsyncVersionTracker
from ragversion.storage import SupabaseStorage

class RealTimeRAGPipeline:
    def __init__(self, vector_db, embedder):
        self.tracker = AsyncVersionTracker(
            storage=SupabaseStorage.from_env()
        )
        self.vector_db = vector_db
        self.embedder = embedder

        # Register change handler
        self.tracker.on_change(self.handle_document_change)

    async def handle_document_change(self, event):
        """Process document changes in real-time."""

        print(f"Processing {event.file_name}...")

        if event.change_type == "deleted":
            # Remove from vector DB
            await self.vector_db.delete(event.document_id)
            print(f"Removed {event.file_name} from vector DB")

        elif event.change_type in ["created", "modified"]:
            # Remove old embeddings if modified
            if event.change_type == "modified":
                await self.vector_db.delete(event.document_id)

            # Load and process document
            content = await self.load_document(event.file_path)
            chunks = self.split_content(content)

            # Generate embeddings
            embeddings = await self.embedder.embed(chunks)

            # Store in vector DB
            await self.vector_db.insert(
                document_id=str(event.document_id),
                texts=chunks,
                embeddings=embeddings,
                metadata={
                    "file_name": event.file_name,
                    "version": event.version_number,
                    "hash": event.content_hash
                }
            )

            print(f"Indexed {event.file_name} (v{event.version_number})")

    async def start_watching(self, directory: str):
        """Start watching directory for changes."""

        await self.tracker.initialize()

        while True:
            # Poll for changes every minute
            await self.tracker.track_directory(directory)
            await asyncio.sleep(60)
```

### Use Case 3: Multi-Source Document Sync

```python
from typing import List
import asyncio
from ragversion import AsyncVersionTracker, SupabaseStorage

class MultiSourceSync:
    def __init__(self):
        self.tracker = AsyncVersionTracker(
            storage=SupabaseStorage.from_env()
        )

    async def sync_all_sources(self):
        """Sync documents from multiple sources."""

        sources = [
            {"path": "/data/documentation", "tags": {"type": "docs"}},
            {"path": "/data/support", "tags": {"type": "support"}},
            {"path": "/data/legal", "tags": {"type": "legal"}},
        ]

        await self.tracker.initialize()

        try:
            results = await asyncio.gather(*[
                self.sync_source(source["path"], source["tags"])
                for source in sources
            ])

            total_changes = sum(r.success_count for r in results)
            print(f"Total changes across all sources: {total_changes}")

        finally:
            await tracker.close()

    async def sync_source(self, path: str, tags: dict):
        """Sync a single source."""

        result = await self.tracker.track_directory(
            path,
            metadata=tags
        )

        print(f"Synced {path}: {result.success_count} changes")
        return result
```

### Use Case 4: Compliance Audit Trail

```python
from ragversion import AsyncVersionTracker, SupabaseStorage
from datetime import datetime, timedelta

class ComplianceAuditor:
    def __init__(self):
        self.tracker = AsyncVersionTracker(
            storage=SupabaseStorage.from_env()
        )

    async def generate_audit_report(self, days: int = 30):
        """Generate compliance audit report."""

        await self.tracker.initialize()

        try:
            # Get all documents
            documents = await self.tracker.list_documents()

            # Audit data
            report = {
                "generated_at": datetime.now(),
                "period_days": days,
                "total_documents": len(documents),
                "changes": []
            }

            # Check each document for changes
            cutoff_date = datetime.now() - timedelta(days=days)

            for doc in documents:
                versions = await self.tracker.list_versions(doc.id)

                recent_changes = [
                    v for v in versions
                    if v.created_at >= cutoff_date
                ]

                if recent_changes:
                    report["changes"].append({
                        "document": doc.file_name,
                        "document_id": str(doc.id),
                        "change_count": len(recent_changes),
                        "changes": [
                            {
                                "version": v.version_number,
                                "type": v.change_type.value,
                                "timestamp": v.created_at.isoformat(),
                                "hash": v.content_hash[:8]
                            }
                            for v in recent_changes
                        ]
                    })

            return report

        finally:
            await self.tracker.close()

    async def verify_document_integrity(self, document_id):
        """Verify document hasn't been tampered with."""

        versions = await self.tracker.list_versions(document_id)

        # Check hash chain
        for i, version in enumerate(versions):
            # Recompute hash from content
            if version.content:
                recomputed_hash = compute_hash(version.content)
                if recomputed_hash != version.content_hash:
                    return {
                        "integrity": "FAILED",
                        "version": version.version_number,
                        "reason": "Hash mismatch"
                    }

        return {"integrity": "VERIFIED"}
```

### Use Case 5: A/B Testing with Document Versions

```python
from ragversion import AsyncVersionTracker, SupabaseStorage
import random

class RAGABTester:
    def __init__(self):
        self.tracker = AsyncVersionTracker(
            storage=SupabaseStorage.from_env()
        )

    async def get_document_for_experiment(
        self,
        document_id,
        experiment_group: str
    ):
        """Get specific version based on experiment group."""

        versions = await self.tracker.list_versions(document_id)

        if experiment_group == "control":
            # Use oldest version
            version = versions[0]
        elif experiment_group == "treatment":
            # Use latest version
            version = versions[-1]
        else:
            # Random version
            version = random.choice(versions)

        content = await self.tracker.get_content(
            document_id,
            version.version_number
        )

        return {
            "content": content,
            "version": version.version_number,
            "metadata": {
                "experiment_group": experiment_group,
                "version_number": version.version_number,
                "hash": version.content_hash
            }
        }
```

---

## Best Practices

### 1. Error Handling

```python
# Always use try/finally for cleanup
async def sync_documents():
    tracker = AsyncVersionTracker(storage=SupabaseStorage.from_env())

    await tracker.initialize()

    try:
        result = await tracker.track_directory("./docs")

        # Check for errors
        if result.failure_count > 0:
            # Log errors
            for error in result.failed:
                logger.error(f"Failed: {error.file_path} - {error.error}")

            # Alert if too many failures
            if result.success_rate < 0.95:
                await alert_team(result)

        return result

    except Exception as e:
        logger.exception("Sync failed")
        raise

    finally:
        await tracker.close()
```

### 2. Resource Management

```python
# Use context manager for automatic cleanup
async def process_documents():
    async with AsyncVersionTracker(storage=storage) as tracker:
        result = await tracker.track_directory("./docs")
        return result
    # Automatically calls close()
```

### 3. Batch Size Optimization

```python
# Adjust workers based on file size
async def smart_batch_processing(directory: str):
    # Count and analyze files
    files = list(Path(directory).rglob("*"))
    total_size = sum(f.stat().st_size for f in files)
    avg_size = total_size / len(files)

    # Adjust workers
    if avg_size < 1_000_000:  # Small files
        workers = 16
    elif avg_size < 10_000_000:  # Medium files
        workers = 8
    else:  # Large files
        workers = 4

    tracker = AsyncVersionTracker(storage=storage)
    result = await tracker.track_directory(
        directory,
        max_workers=workers
    )

    return result
```

### 4. Content Storage Strategy

```python
# Store content selectively
async def selective_storage():
    # Small files: store content
    small_tracker = AsyncVersionTracker(
        storage=storage,
        store_content=True,
        max_file_size_mb=10
    )

    # Large files: track metadata only
    large_tracker = AsyncVersionTracker(
        storage=storage,
        store_content=False,
        max_file_size_mb=100
    )
```

### 5. Monitoring and Observability

```python
import logging
from prometheus_client import Counter, Histogram

# Metrics
documents_tracked = Counter('documents_tracked_total', 'Total documents tracked')
track_duration = Histogram('track_duration_seconds', 'Track operation duration')

async def monitored_tracking(directory: str):
    with track_duration.time():
        tracker = AsyncVersionTracker(storage=storage)

        # Add progress callback
        async def track_progress(current, total):
            documents_tracked.inc()
            logger.info(f"Progress: {current}/{total}")

        result = await tracker.track_directory(
            directory,
            progress_callback=track_progress
        )

        # Log metrics
        logger.info(
            f"Tracked {result.success_count} documents "
            f"in {result.duration_seconds:.2f}s"
        )

        return result
```

### 6. Testing Strategy

```python
# Use MockStorage for testing
import pytest
from ragversion.testing import MockStorage, create_sample_documents

@pytest.mark.asyncio
async def test_document_tracking():
    # Use mock storage
    tracker = AsyncVersionTracker(storage=MockStorage())

    await tracker.initialize()

    # Create sample documents
    docs = create_sample_documents(count=10, file_type="pdf")

    # Test tracking
    for doc in docs:
        change = await tracker.track(doc.path)
        assert change is not None
        assert change.change_type == "created"

    # Verify
    documents = await tracker.list_documents()
    assert len(documents) == 10
```

### 7. Configuration Management

```python
# Use environment-specific configs
import os
from ragversion.config import RAGVersionConfig

def get_config():
    env = os.getenv("ENV", "development")

    if env == "production":
        return RAGVersionConfig.load("config/production.yaml")
    elif env == "staging":
        return RAGVersionConfig.load("config/staging.yaml")
    else:
        return RAGVersionConfig.load("config/development.yaml")

config = get_config()
```

---

## Troubleshooting

### Common Issues

#### Issue 1: "No module named 'ragversion'"

**Cause**: Package not installed

**Solution**:
```bash
pip install ragversion
```

#### Issue 2: "Supabase connection failed"

**Cause**: Invalid credentials or network issues

**Solution**:
```bash
# Check environment variables
echo $SUPABASE_URL
echo $SUPABASE_SERVICE_KEY

# Test connection
ragversion health

# Verify Supabase project is active
# Check firewall/network settings
```

#### Issue 3: "Table 'documents' does not exist"

**Cause**: Database not migrated

**Solution**:
```bash
# Run migration
ragversion migrate

# Copy SQL output and run in Supabase SQL Editor
```

#### Issue 4: "Permission denied" errors

**Cause**: File permission issues

**Solution**:
```bash
# Check file permissions
ls -la /path/to/documents

# Fix permissions
chmod 644 /path/to/documents/*
```

#### Issue 5: "Too many open files"

**Cause**: Too many parallel workers

**Solution**:
```python
# Reduce max_workers
result = await tracker.track_directory(
    directory,
    max_workers=2  # Lower value
)

# Or increase system limit
# ulimit -n 4096
```

#### Issue 6: High memory usage

**Cause**: Storing content of large files

**Solution**:
```python
# Disable content storage
tracker = AsyncVersionTracker(
    storage=storage,
    store_content=False
)

# Or reduce max file size
tracker = AsyncVersionTracker(
    storage=storage,
    max_file_size_mb=10
)
```

### Debug Mode

```python
import logging

# Enable debug logging
logging.basicConfig(
    level=logging.DEBUG,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)

# RAGVersion will output detailed logs
tracker = AsyncVersionTracker(storage=storage)
```

### Health Checks

```python
# Programmatic health check
async def check_system_health():
    tracker = AsyncVersionTracker(storage=storage)

    await tracker.initialize()

    try:
        # Check storage
        healthy = await tracker.storage.health_check()

        if not healthy:
            logger.error("Storage backend unhealthy")
            return False

        # Test tracking
        test_file = create_test_file()
        change = await tracker.track(test_file)

        if change is None:
            logger.error("Tracking test failed")
            return False

        return True

    finally:
        await tracker.close()
```

---

## Architecture Deep Dive

### System Architecture

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    Application Layer                     ‚îÇ
‚îÇ  (Your RAG Application, LangChain, LlamaIndex, etc.)   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                         ‚îÇ
                         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                  RAGVersion API Layer                    ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ         AsyncVersionTracker (Core Engine)         ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  - Change Detection                               ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  - Version Management                             ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  - Event System                                   ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  - Batch Processing                               ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                         ‚îÇ
           ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
           ‚îÇ             ‚îÇ             ‚îÇ
           ‚ñº             ‚ñº             ‚ñº
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ Document ‚îÇ  ‚îÇ  Change  ‚îÇ  ‚îÇ Storage  ‚îÇ
    ‚îÇ Parsers  ‚îÇ  ‚îÇ Detector ‚îÇ  ‚îÇ Backend  ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
           ‚îÇ             ‚îÇ             ‚îÇ
           ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                         ‚îÇ
                         ‚ñº
              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
              ‚îÇ  Supabase/PostgreSQL ‚îÇ
              ‚îÇ   (Storage Layer)    ‚îÇ
              ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Component Details

#### 1. AsyncVersionTracker

Core engine that orchestrates all operations:

```python
class AsyncVersionTracker:
    def __init__(self, storage, ...):
        self.storage = storage
        self.detector = ChangeDetector()
        self.processor = BatchProcessor()
        self.events = EventSystem()

    async def track(self, file_path):
        # 1. Parse document
        content = await self.parser.parse(file_path)

        # 2. Detect changes
        change = await self.detector.detect(content)

        # 3. Store version
        if change:
            await self.storage.create_version(change)

        # 4. Emit event
        if change:
            await self.events.emit(change)

        return change
```

#### 2. Document Parsers

Pluggable parsers for different file types:

```python
class ParserRegistry:
    parsers = {
        ".pdf": PDFParser,
        ".docx": DOCXParser,
        ".txt": TextParser,
        ".md": MarkdownParser,
    }

    def get_parser(self, file_type):
        return self.parsers.get(file_type)
```

#### 3. Change Detector

Content-based change detection:

```python
class ChangeDetector:
    def compute_hash(self, content: bytes) -> str:
        return hashlib.sha256(content).hexdigest()

    async def detect_change(self, file_path):
        # Get current content
        current_content = await read_file(file_path)
        current_hash = self.compute_hash(current_content)

        # Get previous version
        previous = await storage.get_latest_version(file_path)

        # Compare
        if not previous:
            return ChangeType.CREATED
        elif current_hash != previous.content_hash:
            return ChangeType.MODIFIED
        else:
            return None  # No change
```

#### 4. Storage Backend

Abstract interface with Supabase implementation:

```python
class StorageBackend(ABC):
    @abstractmethod
    async def create_document(self, doc: Document) -> Document:
        pass

    @abstractmethod
    async def create_version(self, version: Version) -> Version:
        pass

    @abstractmethod
    async def get_latest_version(self, doc_id: UUID) -> Version:
        pass

class SupabaseStorage(StorageBackend):
    def __init__(self, url, key):
        self.client = create_client(url, key)

    async def create_document(self, doc):
        result = await self.client.table("documents").insert(doc).execute()
        return result.data[0]
```

#### 5. Batch Processor

Parallel processing with error handling:

```python
class BatchProcessor:
    async def process_batch(self, files, max_workers=4):
        semaphore = asyncio.Semaphore(max_workers)

        async def process_one(file):
            async with semaphore:
                try:
                    return await tracker.track(file)
                except Exception as e:
                    return FileProcessingError(file, e)

        tasks = [process_one(f) for f in files]
        results = await asyncio.gather(*tasks)

        return BatchResult(results)
```

#### 6. Event System

Async callback system:

```python
class EventSystem:
    def __init__(self):
        self.callbacks = []

    def on_change(self, callback):
        self.callbacks.append(callback)

    async def emit(self, event):
        await asyncio.gather(*[
            cb(event) for cb in self.callbacks
        ])
```

### Database Schema

```sql
-- Documents table
CREATE TABLE documents (
    id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    file_path TEXT NOT NULL,
    file_name TEXT NOT NULL,
    file_type TEXT NOT NULL,
    file_size BIGINT NOT NULL,
    content_hash TEXT NOT NULL,
    version_count INTEGER DEFAULT 0,
    created_at TIMESTAMPTZ DEFAULT NOW(),
    updated_at TIMESTAMPTZ DEFAULT NOW()
);

-- Versions table
CREATE TABLE versions (
    id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    document_id UUID REFERENCES documents(id) ON DELETE CASCADE,
    version_number INTEGER NOT NULL,
    content_hash TEXT NOT NULL,
    change_type TEXT NOT NULL,
    file_size BIGINT NOT NULL,
    content BYTEA,
    created_at TIMESTAMPTZ DEFAULT NOW(),
    UNIQUE(document_id, version_number)
);

-- Indexes
CREATE INDEX idx_documents_file_path ON documents(file_path);
CREATE INDEX idx_documents_updated_at ON documents(updated_at DESC);
CREATE INDEX idx_versions_document_id ON versions(document_id);
CREATE INDEX idx_versions_created_at ON versions(created_at DESC);
```

### Performance Characteristics

#### Scalability

- **Documents**: Tested with 100,000+ documents
- **Versions**: Tested with 1,000+ versions per document
- **Throughput**: 100+ documents/second (depends on file size)
- **Parallel workers**: Scales linearly up to 16 workers

#### Storage Requirements

- **Metadata only**: ~1KB per version
- **With content**: Varies by file size + compression
- **Compression ratio**: Typically 60-80% size reduction with gzip

#### Network Efficiency

- **Batch operations**: Single network round-trip for multiple documents
- **Query optimization**: Indexes on hot paths
- **Content compression**: Reduces network transfer

---

## Conclusion

RAGVersion is a production-ready solution for managing document versions in RAG applications. It solves the critical problem of keeping vector databases synchronized with changing source documents, while providing:

- **Cost savings** through incremental updates
- **Reliability** with resilient error handling
- **Observability** with complete audit trails
- **Flexibility** through framework integrations
- **Performance** with async architecture

### Getting Help

- **GitHub Issues**: https://github.com/sourangshupal/ragversion/issues
- **Discussions**: https://github.com/sourangshupal/ragversion/discussions
- **PyPI**: https://pypi.org/project/ragversion/

### Contributing

We welcome contributions! See [CONTRIBUTING.md](CONTRIBUTING.md) for guidelines.

### License

MIT License - see [LICENSE](LICENSE) for details.

---

**Built with ‚ù§Ô∏è for the RAG community**
